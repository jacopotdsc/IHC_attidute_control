{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions import Normal\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import struct\n",
        "import socket\n",
        "\n",
        "import matlab.engine\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "global_var = {\n",
        "\n",
        "    # Connection\n",
        "    'ip_address': '127.0.0.1',                  # Ip to connect\n",
        "    'port_input': 8280,                         # Port where matlab send data\n",
        "    'port_output': 8281,                        # Port where matlab receive data\n",
        "    'port_f_input': 8282,                       # DEPRECATED: Same but for feedback branch\n",
        "    'port_f_output': 8283,                      # DEPRECATED: Same but for feedback branch\n",
        "    'buffer_size': 32,                          # Number of bytes to read \n",
        "    'connection_dim_input': 4,                  # Number of data to read -> used to unpack from byte, recevied data are byte\n",
        "    'connection_dim_output': 1,                 # Number of data in output -> used to pack to byte and sedn to matlab\n",
        "    'stop_flag': [-999, -999, -999, -999],      # DEPRECATED\n",
        "    'use_pid': True,                            # DEPRECATED\n",
        "        \n",
        "    # Matlab\n",
        "    'matlab_path': r'C:/Users/pc/Desktop/Artificial Intellingence & Robotics/1y-2s/Intelligent and hybrid control/IHC_attidute_control/control_schemas',\n",
        "    'simulation_name': 'pid_reinforcement.slx', \n",
        "    'simulation_time': 40.0,    # Time of the simulation\n",
        "    'open_GUI': False,          # If True, it open the simulink schema\n",
        "\n",
        "    # Dataset\n",
        "    'max_len_dataset': 10000,\n",
        "\n",
        "    # Feed Forward Network \n",
        "    'input_dim': 4,               # Dimension of input layer\n",
        "    'output_dim': 1,              # Dimension of output layer\n",
        "    'hidden_dim': 10,             # Dimension of hidden layer\n",
        "    'standard_deviation': 0,\n",
        "    'output_range': 10,           # Max control value\n",
        "    'bias': True,                \n",
        "    'learning_rate': 0.001,       \n",
        "    'model_name': 'model.pt',     # Used to save weights\n",
        "    'model_path': 'weights/',     # Folder where save weights\n",
        "\n",
        "    # Train\n",
        "    'reward_control_factor': 0.01,  # facotr which damp the control. if 0 -> r=-e^2\n",
        "    'batch_size': 32,             # Batch size for training\n",
        "    'start_train_size': 1,        # DEPRECATED: Minium number of sample to traine\n",
        "    'n_episode': 25,                   \n",
        "    'epoch_size': 10,             # Number of iteration on an epoch\n",
        "    'frequency_update': 32,       # Frequency update of the network\n",
        "    'threshold_error': 100.0,    \n",
        "    'max_reward': -3, \n",
        "    'epsilon': 0.3,  \n",
        "    'max_abs_control': 5.0, \n",
        "\n",
        "    # Plots\n",
        "    'plots_path': 'plots/'        # Folder where save plots\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "class Color:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "    MAGENTA = '\\033[95m'\n",
        "    CYAN = '\\033[96m'\n",
        "    WHITE = '\\033[97m'\n",
        "    RESET = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actor-Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lr, bias):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.loss_fnc = nn.MSELoss()\n",
        "        \n",
        "        self.layer_1 = nn.Linear(\n",
        "            in_features=self.input_dim,\n",
        "            out_features=self.hidden_dim,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.layer_2 = nn.Linear(\n",
        "            in_features=self.hidden_dim,\n",
        "            out_features=self.hidden_dim*2,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.layer_3 = nn.Linear(\n",
        "            in_features=self.hidden_dim*2,\n",
        "            out_features=self.hidden_dim,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.layer_out= nn.Linear(\n",
        "            in_features=self.hidden_dim,\n",
        "            out_features=1,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        state = self.layer_1(state)\n",
        "        state = self.relu(state)\n",
        "\n",
        "        state = self.layer_2(state)\n",
        "        state = self.relu(state) \n",
        "\n",
        "        state = self.layer_3(state)\n",
        "        state = self.relu(state) \n",
        "\n",
        "        state = self.layer_out(state)\n",
        "        state = self.relu(state) \n",
        "        \n",
        "        return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, lr, bias):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.loss_fnc = nn.MSELoss()\n",
        "        \n",
        "        \n",
        "        self.layer_1 = nn.Linear(\n",
        "            in_features=self.input_dim,\n",
        "            out_features=self.hidden_dim,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.layer_2 = nn.Linear(\n",
        "            in_features=self.hidden_dim,\n",
        "            out_features=self.hidden_dim*2,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.layer_3 = nn.Linear(\n",
        "            in_features=self.hidden_dim*2,\n",
        "            out_features=self.hidden_dim,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "\n",
        "        self.layer_out = nn.Linear(\n",
        "            in_features=self.hidden_dim,\n",
        "            out_features=self.output_dim,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        state = self.layer_1(state)\n",
        "        state = self.relu(state)\n",
        "\n",
        "        state = self.layer_2(state)\n",
        "        state = self.relu(state)\n",
        "\n",
        "        state = self.layer_3(state)\n",
        "        state = self.relu(state)\n",
        "        \n",
        "        state = self.layer_out(state)\n",
        "        mu = self.relu(state)\n",
        "\n",
        "        return mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, bias, std, lr, model_name):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.critic = Critic(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            bias=bias,\n",
        "            lr=lr\n",
        "            )\n",
        "\n",
        "        self.actor = Actor(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            output_dim=output_dim,\n",
        "            bias=bias,\n",
        "            lr=lr\n",
        "            )\n",
        "\n",
        "        self.log_std = nn.Parameter(torch.ones(1, output_dim) * std)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        #self.loss_fnc = nn.MSELoss()\n",
        "        #self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        value = self.critic(x)\n",
        "        mu    = self.actor(x)\n",
        "        std = self.log_std.exp() #.expand_as(mu)\n",
        "\n",
        "        action_distribution  = Normal(mu, std)\n",
        "        \n",
        "        raw_action = action_distribution.sample()\n",
        "        raw_log_action = action_distribution.log_prob(raw_action) # log probability of an action given the distribution\n",
        "\n",
        "        action = self.tanh(raw_action)*global_var['max_abs_control']\n",
        "        log_action = raw_log_action - torch.log( 1 - self.tanh(raw_action)**2 + 0.001  ) + torch.log( torch.tensor(global_var['max_abs_control']) )\n",
        "\n",
        "        # In statistic  given a random variable X, a transformation Y = f(X) is another random variable,\n",
        "        # the probability of Y is Py(Y) = Px( f(y)^-1 )*| D[f(y)^-1]_y |\n",
        "\n",
        "        # the transformation of log probability with tansh is the following:\n",
        "        # log( P ) = log( Pa ) - log( D[ tanh(a) ]_a ), where D[ tanh(a) ]_a = 1 - tanh(a)^2\n",
        "        # log( Pa ) is the probability density of a with the normal distribution\n",
        "\n",
        "        return action, value, action_distribution, log_action\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        name = self.model_name\n",
        "        torch.save(self.state_dict(), name )\n",
        "        #print(f\"{Color.MAGENTA}Saved: {name}{Color.RESET}\")\n",
        "\n",
        "    def load(self):\n",
        "        name = self.model_name\n",
        "        try:\n",
        "            self.load_state_dict(torch.load(name) )\n",
        "            print(f\"{Color.MAGENTA}loaded: {name}{Color.RESET}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{Color.RED}Model not loaded{Color.RESET}\")\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_socket(ip_address, port_input):\n",
        "\n",
        "    print(f\"{Color.YELLOW}ip: {ip_address}, port_input: {port_input}{Color.RESET}\")\n",
        "\n",
        "    try:\n",
        "        socket_nn_input = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        socket_nn_input.bind( (ip_address, port_input) )\n",
        "        socket_nn_input.listen(1)\n",
        "\n",
        "        print(f\"{Color.BOLD}{Color.GREEN}Sockets listener created{Color.RESET}\")\n",
        "\n",
        "        return socket_nn_input\n",
        "    except Exception as e:\n",
        "        print(f\"{Color.RED}An error occurred: {e}{Color.RESET}\")\n",
        "\n",
        "        if 'socket_nn_input' in locals():\n",
        "            socket_nn_input.close()\n",
        "            \n",
        "        return None, None\n",
        "    \n",
        "def accept_connection(socket_server):\n",
        "    connection, address = socket_server.accept()\n",
        "    print(f\"{Color.GREEN}Connection accepted!{Color.RESET}\")\n",
        "    return connection, address\n",
        "\n",
        "def receive_data(connection, buffer_size, dim_input):\n",
        "    \n",
        "    expected_bytes = buffer_size  # Size for one double\n",
        "    no_data_flag = False\n",
        "    data = b''\n",
        "    while len(data) < expected_bytes:\n",
        "        more_data = connection.recv(expected_bytes - len(data))\n",
        "\n",
        "        if not more_data:\n",
        "            print(\"\\nNo data received. Ending connection.\")\n",
        "            no_data_flag = True\n",
        "            break\n",
        "\n",
        "        data += more_data\n",
        "    data = list(struct.unpack(f'!{str(dim_input)}d', data))  # Unpack one double\n",
        "    \n",
        "    return data, no_data_flag\n",
        "\n",
        "def receive_data_excpt(connection, buffer_size, dim_input, stop_flag):\n",
        "    \n",
        "    expected_bytes = 8# buffer_size  # Size for one double\n",
        "    data = b''\n",
        "    #connection.settimeout(5.0)  # Set timeout to 5 seconds\n",
        "\n",
        "    try:\n",
        "        while len(data) < expected_bytes:\n",
        "            #more_data = connection.recv(expected_bytes - len(data))\n",
        "            more_data = connection.recv(expected_bytes)\n",
        "            if not more_data:\n",
        "                # No more data is available, break the loop\n",
        "                break\n",
        "            data += more_data\n",
        "        try:\n",
        "            #data = list(struct.unpack(f'!{str(dim_input)}d', data)) \n",
        "            data = list(struct.unpack(f'!d', data)) \n",
        "        except Exception as e:\n",
        "            print(f\"\\n{e}\")\n",
        "            print(f\"{Color.RED}\\nProblem with unpacking, error: {e}{Color.RESET}\")\n",
        "            print(f\"{Color.RED}May be due because return empty string when nothing is receive for a certain time{Color.RESET}\")\n",
        "            return stop_flag\n",
        "        \n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"\\n{e}\")\n",
        "        if isinstance(e, socket.timeout):\n",
        "            print(f\"{Color.RED}\\nTimeout error: No data received within the timeout period{Color.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Color.RED}\\nOther exception occurred: {e}{Color.RESET}\")\n",
        "            print(f\"{Color.RED}Maybe due to some other error in the code{Color.RESET}\")\n",
        "            \n",
        "        print(f\"{Color.RED}May be due because return empty string when nothing is receive for a certain time{Color.RESET}\")\n",
        "        print(f\"{Color.BLUE}\\nNo data received within the timeout period, maybe some error of code{Color.RESET}\")\n",
        "       \n",
        "        return stop_flag\n",
        "    \n",
        "    connection.settimeout(None)\n",
        "    return data\n",
        "\n",
        "def send_data(connection, message, dim_output):\n",
        "    try:\n",
        "        #message_to_send = struct.pack(f'!{str(dim_output)}d', *message)  \n",
        "        message_to_send = struct.pack(f'!{str(dim_output)}d', message)\n",
        "        connection.sendall(message_to_send)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\rError sending float: {e}\", end='')\n",
        "\n",
        "def close_connections(*to_close):\n",
        "    try:\n",
        "        for c in to_close:\n",
        "            c.close()\n",
        "    except Exception as e:\n",
        "        print(f\"{Color.RED}\\nFailed to close connection: verified the follwing error:\\n{e}{Color.RESET}\")\n",
        "        return\n",
        "\n",
        "    print(f\"{Color.GREEN}\\nSockets closed!{Color.RESET}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_simulink(model_name, matlab_path, simulation_time):\n",
        "\n",
        "    model_name = model_name.split('.')[0]\n",
        "\n",
        "    print(f\"\\nStarting Matlab engine ... \", end='')\n",
        "    future = matlab.engine.start_matlab(background=True)\n",
        "    eng = future.result()  \n",
        "    print(f\"Started!\")\n",
        "\n",
        "    eng.cd(matlab_path)\n",
        "    print(\"MATLAB working directory:\", eng.pwd())\n",
        "\n",
        "    if global_var['open_GUI'] == False:\n",
        "        eng.load_system(model_name)\n",
        "    else:\n",
        "        eng.open_system(model_name, nargout=0)\n",
        "        \n",
        "    loaded_model = eng.bdroot()\n",
        "    print(f\"The currently loaded model is: {loaded_model}\")\n",
        "\n",
        "    eng.set_param(model_name, 'StartTime', '0', nargout=0)\n",
        "    eng.set_param(model_name, 'StopTime', str(simulation_time), nargout=0)\n",
        "    eng.set_param(model_name, 'SimulationCommand', 'start', nargout=0)\n",
        "\n",
        "    return eng\n",
        "\n",
        "def run_simulink_with_gui(model_name, matlab_path, simulation_time):\n",
        "\n",
        "    model_name = model_name.split('.')[0]\n",
        "\n",
        "    print(f\"\\nStarting Matlab engine ... \", end='')\n",
        "    future = matlab.engine.start_matlab(background=True)\n",
        "    eng = future.result()  \n",
        "    print(f\"Started!\")\n",
        "\n",
        "    eng.cd(matlab_path)\n",
        "    print(\"MATLAB working directory:\", eng.pwd())\n",
        "\n",
        "    eng.open_system(model_name, nargout=0)\n",
        "    loaded_model = eng.bdroot()\n",
        "\n",
        "    print(f\"The currently loaded model is: {loaded_model}\")\n",
        "\n",
        "    eng.set_param(model_name, 'StartTime', '0', nargout=0)\n",
        "    eng.set_param(model_name, 'StopTime', str(simulation_time), nargout=0)\n",
        "    eng.set_param(model_name, 'SimulationCommand', 'start', nargout=0)\n",
        "\n",
        "    return eng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.5)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ControllDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        #self.data = []\n",
        "        self.data = deque(maxlen=global_var['max_len_dataset'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        state, label = item[:-1], item[-1]\n",
        "        return state, label\n",
        "\n",
        "    def add_data(self, new_data):\n",
        "        self.data.append(new_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, states, rewards, actions, log_action, next_states):\n",
        "\n",
        "    # converting array of tensor inot a tensor of tensor to create the stack for the batch\n",
        "    states = torch.stack( states, dim=0 )\n",
        "    rewards = torch.stack( rewards, dim=0)\n",
        "    actions = torch.stack( actions, dim=0)\n",
        "    log_action = torch.stack( log_action, dim=0)\n",
        "    next_states = torch.stack( next_states, dim=0)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss = 0\n",
        "    gamma = 0.99\n",
        "\n",
        "    model.critic.optimizer.zero_grad()\n",
        "    model.actor.optimizer.zero_grad()\n",
        "\n",
        "    ##### start calculating loss #####\n",
        "\n",
        "    #dist, values = model(states)\n",
        "    _, values, _, _ = model(states)\n",
        "\n",
        "    value_next_state = model.critic(next_states).detach()\n",
        "    label = rewards + gamma * value_next_state\n",
        "    delta =  label - values \n",
        "\n",
        "    critic_loss = model.critic.loss_fnc(delta, torch.zeros(delta.shape) ) \n",
        "    actor_loss = -torch.sum(delta * log_action )\n",
        "    actor_loss = actor_loss.mean()\n",
        "\n",
        "    #print(f\"Delta: {delta.mean():.3f}, actions: {actions.mean():.3f}, log_act: {log_action.mean():.3f}, Critic: {critic_loss:.3f}, Actor: {actor_loss:.3f}\")\n",
        "\n",
        "    ##### End calculating loss #####\n",
        "\n",
        "\n",
        "    actor_loss.backward(retain_graph=True)\n",
        "    critic_loss.backward()\n",
        "\n",
        "    model.critic.optimizer.step()\n",
        "    model.actor.optimizer.step()\n",
        "\n",
        "    model.save()\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = critic_loss + actor_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_statistics(history_error, history_desired_input, history_reference_model, history_controlled_object, history_mean_error, history_reward, name):\n",
        "    time = np.linspace(0, int(global_var['simulation_time']), len(history_error))\n",
        "    time_reward = np.linspace(0, int(global_var['simulation_time']), len(history_reward))\n",
        "\n",
        "    fig, axs = plt.subplots(2,3,figsize=(18, 10))\n",
        "\n",
        "    axs[0, 0].plot(time, history_desired_input)\n",
        "    axs[0, 0].set_title('Desired input')\n",
        "    axs[0, 0].grid(True)\n",
        "\n",
        "    axs[0, 1].plot(time, history_reference_model)\n",
        "    axs[0, 1].set_title('Reference model')\n",
        "    axs[0, 1].grid(True)\n",
        "\n",
        "    axs[1, 0].plot(time, history_controlled_object)\n",
        "    axs[1, 0].set_title('Controlled object')\n",
        "    axs[1, 0].grid(True)\n",
        "\n",
        "    axs[1, 1].plot(time_reward, history_reward)\n",
        "    axs[1, 1].set_title('Reward')\n",
        "    axs[1, 1].grid(True)\n",
        "\n",
        "    axs[0, 2].plot(time, history_error)\n",
        "    axs[0, 2].set_title('Error')\n",
        "    axs[0, 2].grid(True)\n",
        "\n",
        "\n",
        "    axs[1, 2].plot(time, history_mean_error)\n",
        "    axs[1, 2].set_title('Abs mean error - last 50')\n",
        "    axs[1, 2].grid(True)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_name = name + '_' + 'range_' + str(global_var['max_abs_control']) + '_sim_' + global_var['simulation_name'].split('.')[0] + '_' + str(int(global_var['simulation_time']))\n",
        "    print(fig_name)\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.savefig(global_var['plots_path'] + fig_name)\n",
        "    plt.show(block=False)\n",
        "\n",
        "    \n",
        "    #plt.pause(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_reward(error, control):\n",
        "    error = error / global_var['threshold_error']\n",
        "    control = control / global_var['max_abs_control']\n",
        "\n",
        "    reward = error**2 + global_var['reward_control_factor']*control**2\n",
        "    reward = -reward\n",
        "\n",
        "    reward = max(reward, -abs(global_var['max_reward']) )\n",
        "    \n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_episode( connection_receiver, \n",
        "                  connection_sender, \n",
        "                  network, \n",
        "                  history_desired_input, \n",
        "                  history_reference_model,\n",
        "                  history_controlled_object,\n",
        "                  history_error,\n",
        "                  history_mean_error,\n",
        "                  history_reward,\n",
        "                  mean_error_window,\n",
        "                  train_flag\n",
        "                  ):\n",
        "    \n",
        "    n_received = 0\n",
        "    n_sent = 0\n",
        "    n_train = 0\n",
        "    mean_loss = 0\n",
        "    n_random = 0\n",
        "    penalize_flag = False\n",
        "    \n",
        "    states_array = []\n",
        "    actions_array = []\n",
        "    log_prob_array = []\n",
        "    rewards_array = []\n",
        "    next_states_array = []\n",
        "\n",
        "    loss_array = []\n",
        "\n",
        "    while True:\n",
        "\n",
        "        raw_data, _ = receive_data(\n",
        "            connection=connection_receiver, \n",
        "            buffer_size=global_var['buffer_size'],\n",
        "            dim_input=global_var['connection_dim_input']\n",
        "            )\n",
        "        n_received += 1\n",
        "        \n",
        "       \n",
        "        history_controlled_object.append(raw_data[0])\n",
        "        history_desired_input.append(raw_data[1])\n",
        "        history_reference_model.append(raw_data[2])\n",
        "        history_error.append(raw_data[-1])\n",
        "        history_mean_error.append(abs(np.mean(history_error[-mean_error_window:])))\n",
        "        \n",
        "    \n",
        "        if abs(raw_data[-1]) > global_var['threshold_error'] and n_received > 1: # end episode if diverging\n",
        "            penalize_flag = True\n",
        "        \n",
        "        #data = raw_data.detach().clone().requires_grad_(True)\n",
        "        raw_data = torch.tensor(raw_data, requires_grad=False)\n",
        "\n",
        "\n",
        "        if len(next_states_array) > 1 and ( len(next_states_array) % global_var['frequency_update'] ) == 0 and train_flag == True:\n",
        "            \n",
        "            current_loss = 0\n",
        "            mean_loss = 0\n",
        "            \n",
        "            current_loss = train(\n",
        "                model=network,\n",
        "                states=states_array[-global_var['frequency_update']:],\n",
        "                actions=actions_array[-global_var['frequency_update']:],\n",
        "                log_action=log_prob_array[-global_var['frequency_update']:],\n",
        "                rewards=rewards_array[-global_var['frequency_update']:],\n",
        "                next_states=next_states_array[-global_var['frequency_update']:]\n",
        "            )  \n",
        "            n_train += 1\n",
        "            #mean_loss = (current_loss + mean_loss).mean()\n",
        "            loss_array.append(current_loss)\n",
        "            mean_loss = torch.mean( torch.stack(loss_array, dim=0))\n",
        "\n",
        "            if penalize_flag == True: # end episode if diverging\n",
        "                penalize_flag = False\n",
        "                print(f\"{Color.RED}\\n---- Error too high, Restarting episode{Color.RESET}\")\n",
        "                raise Exception(\"Error too high, Restarting episode\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            network_input = raw_data #[: -1]\n",
        "            action, value, action_distribution, log_action = network(network_input)\n",
        "            #action = action_distribution.sample()\n",
        "            #log_action = action_distribution.log_prob(action)   # log probability to have taken this action\n",
        "\n",
        "        current_reward = compute_reward( raw_data[-1], action )\n",
        "  \n",
        "\n",
        "        raw_data.requires_grad_(True)\n",
        "        action.requires_grad_(True)\n",
        "        current_reward.requires_grad_(True)\n",
        "\n",
        "        if penalize_flag == True:\n",
        "            current_reward = torch.tensor([[-5]], dtype=torch.float, requires_grad=True)\n",
        "        \n",
        "        history_reward.append(current_reward.item())\n",
        "            \n",
        "\n",
        "        states_array.append( raw_data )\n",
        "        #actions_array.append( action )\n",
        "        log_prob_array.append(log_action)\n",
        "        rewards_array.append( current_reward )\n",
        "        \n",
        "        if not ( states_array == []):\n",
        "            current_next_state = states_array[-1].clone()\n",
        "            current_next_state.requires_grad_(True)\n",
        "            next_states_array.append( current_next_state )\n",
        "\n",
        "        \n",
        "        if random.random() < global_var['epsilon'] and train_flag == True:\n",
        "            action = torch.rand( action.shape )\n",
        "            action = torch.clip(action, min=-global_var['max_abs_control'], max=global_var['max_abs_control'])\n",
        "            action = action\n",
        "            n_random += 1\n",
        "        else:\n",
        "            action = action\n",
        "        \n",
        "        actions_array.append( action )\n",
        "\n",
        "        action = action.item()\n",
        "    \n",
        "        send_data(\n",
        "            connection=connection_sender, \n",
        "            message=action, \n",
        "            dim_output=global_var['connection_dim_output']\n",
        "            )\n",
        "        n_sent += 1\n",
        "\n",
        "        output_line = (\n",
        "            f\"\\rreceived: {n_received}, sent: {n_sent}, n_train: {n_train}, \"\n",
        "            f\"error: {raw_data[-1]:.4f}, reward : {current_reward.item():.4f}, mean_error_{str(mean_error_window)}: {np.mean(history_error[-mean_error_window:]):.4f} ,mean loss: {mean_loss:.4f}{' ' * 50}\"\n",
        "        )\n",
        "        \n",
        "        print(output_line, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialization_connection_and_simulink( ip_address, \n",
        "                                            port_input, \n",
        "                                            port_output,\n",
        "                                            simulation_name,\n",
        "                                            simulation_time,\n",
        "                                            matlab_path\n",
        "                                            ):\n",
        "    receiver_socket = setup_socket(\n",
        "        ip_address=ip_address, \n",
        "        port_input=port_input\n",
        "        )\n",
        "\n",
        "    send_socket = setup_socket(\n",
        "        ip_address=ip_address, \n",
        "        port_input=port_output\n",
        "        )\n",
        "        \n",
        "\n",
        "    #eng = run_simulink_with_gui(model_name, eng_path, simulation_time)\n",
        "    \n",
        "    eng = run_simulink(\n",
        "        model_name=simulation_name, \n",
        "        matlab_path=matlab_path, \n",
        "        simulation_time=simulation_time\n",
        "        )\n",
        "\n",
        "\n",
        "    print(f\"{Color.CYAN}\\nWaiting someone to connect ...{Color.RESET}\")\n",
        "    connection_receiver, addr = accept_connection(receiver_socket)\n",
        "    connection_sender, addr = accept_connection(send_socket)\n",
        "\n",
        "    return receiver_socket, send_socket, connection_receiver, connection_sender, eng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reinforcement_training_loop(\n",
        "        network,\n",
        "        history_desired_input,\n",
        "        history_reference_model,\n",
        "        history_controlled_object,\n",
        "        history_error,\n",
        "        history_mean_error,\n",
        "        mean_error_window,\n",
        "        history_reward, \n",
        "):    \n",
        "    try:\n",
        "\n",
        "        receiver_socket, send_socket, connection_receiver, connection_sender, eng = initialization_connection_and_simulink(\n",
        "                                                                                                                        ip_address=global_var['ip_address'],\n",
        "                                                                                                                        port_input=global_var['port_input'],\n",
        "                                                                                                                        port_output=global_var['port_output'],\n",
        "                                                                                                                        simulation_name=global_var['simulation_name'],\n",
        "                                                                                                                        simulation_time=global_var['simulation_time'],\n",
        "                                                                                                                        matlab_path=global_var['matlab_path']\n",
        "                                                                                                                    )\n",
        "        \n",
        "        play_episode(\n",
        "            connection_receiver=connection_receiver, \n",
        "            connection_sender=connection_sender, \n",
        "            network=network, \n",
        "            history_desired_input=history_desired_input, \n",
        "            history_reference_model=history_reference_model,\n",
        "            history_controlled_object=history_controlled_object,\n",
        "            history_error=history_error,\n",
        "            history_mean_error=history_mean_error,\n",
        "            history_reward=history_reward,\n",
        "            mean_error_window=mean_error_window,\n",
        "            train_flag = True\n",
        "            )\n",
        "       \n",
        "        \n",
        "    except Exception as e:\n",
        "        print()\n",
        "        print(e)\n",
        "        pass\n",
        "\n",
        "    finally:\n",
        "\n",
        "        close_connections(\n",
        "            connection_receiver,\n",
        "            connection_sender,\n",
        "            send_socket,\n",
        "            receiver_socket,\n",
        "        )\n",
        "\n",
        "        eng.quit()\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_reinforcement_loop(\n",
        "        network,\n",
        "        history_desired_input,\n",
        "        history_reference_model,\n",
        "        history_controlled_object,\n",
        "        history_error,\n",
        "        history_mean_error,\n",
        "        history_reward,\n",
        "        mean_error_window,\n",
        "): \n",
        "    for e in range(global_var['n_episode']):\n",
        "        print(f\"\\rn_episode: {e}\\n\", end=\"\")\n",
        "        reinforcement_training_loop(\n",
        "                network=network,\n",
        "                history_desired_input=history_desired_input,\n",
        "                history_reference_model=history_reference_model,\n",
        "                history_controlled_object=history_controlled_object,\n",
        "                history_error=history_error,\n",
        "                history_mean_error=history_mean_error,\n",
        "                history_reward=history_reward,\n",
        "                mean_error_window=mean_error_window,\n",
        "        )\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ActorCritic(\n",
              "  (critic): Critic(\n",
              "    (relu): ReLU()\n",
              "    (tanh): Tanh()\n",
              "    (loss_fnc): MSELoss()\n",
              "    (layer_1): Linear(in_features=4, out_features=10, bias=True)\n",
              "    (layer_2): Linear(in_features=10, out_features=20, bias=True)\n",
              "    (layer_3): Linear(in_features=20, out_features=10, bias=True)\n",
              "    (layer_out): Linear(in_features=10, out_features=1, bias=True)\n",
              "  )\n",
              "  (actor): Actor(\n",
              "    (relu): ReLU()\n",
              "    (loss_fnc): MSELoss()\n",
              "    (layer_1): Linear(in_features=4, out_features=10, bias=True)\n",
              "    (layer_2): Linear(in_features=10, out_features=20, bias=True)\n",
              "    (layer_3): Linear(in_features=20, out_features=10, bias=True)\n",
              "    (layer_out): Linear(in_features=10, out_features=1, bias=True)\n",
              "  )\n",
              "  (tanh): Tanh()\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "network = ActorCritic(  \n",
        "                        input_dim=global_var['input_dim'],\n",
        "                        output_dim=global_var['output_dim'],\n",
        "                        hidden_dim=global_var['hidden_dim'],\n",
        "                        bias=global_var['bias'],\n",
        "                        #output_range=global_var['output_range'],\n",
        "                        std=global_var['standard_deviation'],\n",
        "                        lr=global_var['learning_rate'],\n",
        "                        model_name=global_var['model_path']+global_var['model_name']\n",
        "                    )\n",
        "\n",
        "network.eval()\n",
        "#network.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://simple-pid.readthedocs.io/en/latest/reference.html\n",
        "\n",
        "dataset = ControllDataset()\n",
        "n_received = 0\n",
        "n_sent = 0\n",
        "previous_error = 0\n",
        "\n",
        "history_desired_input = []\n",
        "history_reference_model = []\n",
        "history_controlled_object = []\n",
        "history_error = []\n",
        "history_mean_error = []\n",
        "history_reward = []\n",
        "\n",
        "mean_error_window = 50\n",
        "mean_loss = 0\n",
        "n_train = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_episode: 0\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1664, sent: 1664, n_train: 51, error: 108.8851, reward : -5.0000, mean_error_50: 98.8878 ,mean loss: 70.4593                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 1\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1472, sent: 1472, n_train: 45, error: -102.1041, reward : -5.0000, mean_error_50: -91.3134 ,mean loss: 62.1045                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 2\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1504, sent: 1504, n_train: 46, error: -99.8870, reward : -1.0038, mean_error_50: -90.2046 ,mean loss: 58.4519                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 3\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1760, sent: 1760, n_train: 54, error: 102.3965, reward : -5.0000, mean_error_50: 92.4419 ,mean loss: 61.1833                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 4\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1600, sent: 1600, n_train: 49, error: -109.3182, reward : -5.0000, mean_error_50: -98.1622 ,mean loss: 64.6593                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 5\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1600, sent: 1600, n_train: 49, error: -111.0890, reward : -5.0000, mean_error_50: -99.9784 ,mean loss: 68.7718                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 6\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1440, sent: 1440, n_train: 44, error: -113.1969, reward : -5.0000, mean_error_50: -102.3662 ,mean loss: 77.8315                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 7\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1408, sent: 1408, n_train: 43, error: -103.9879, reward : -5.0000, mean_error_50: -92.9956 ,mean loss: 66.5208                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 8\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1632, sent: 1632, n_train: 50, error: -109.1054, reward : -5.0000, mean_error_50: -97.6148 ,mean loss: 63.0100                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 9\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1888, sent: 1888, n_train: 58, error: -103.0856, reward : -5.0000, mean_error_50: -93.1377 ,mean loss: 51.1941                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 10\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... Started!\n",
            "MATLAB working directory: C:\\Users\\pc\\Desktop\\Artificial Intellingence & Robotics\\1y-2s\\Intelligent and hybrid control\\IHC_attidute_control\\control_schemas\n",
            "The currently loaded model is: pid_reinforcement\n",
            "\u001b[96m\n",
            "Waiting someone to connect ...\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "\u001b[92mConnection accepted!\u001b[0m\n",
            "received: 1440, sent: 1440, n_train: 44, error: -108.1240, reward : -5.0000, mean_error_50: -97.7495 ,mean loss: 70.8375                                                  \u001b[91m\n",
            "---- Error too high, Restarting episode\u001b[0m\n",
            "\n",
            "Error too high, Restarting episode\n",
            "\u001b[92m\n",
            "Sockets closed!\u001b[0m\n",
            "n_episode: 11\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8280\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\u001b[93mip: 127.0.0.1, port_input: 8281\u001b[0m\n",
            "\u001b[1m\u001b[92mSockets listener created\u001b[0m\n",
            "\n",
            "Starting Matlab engine ... "
          ]
        }
      ],
      "source": [
        "play_reinforcement_loop(\n",
        "        network=network,\n",
        "        history_desired_input=history_desired_input,\n",
        "        history_reference_model=history_reference_model,\n",
        "        history_controlled_object=history_controlled_object,\n",
        "        history_error=history_error,\n",
        "        history_mean_error=history_mean_error,\n",
        "        history_reward=history_reward,\n",
        "        mean_error_window=mean_error_window,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "network.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_statistics(\n",
        "    history_error=history_error,\n",
        "    history_desired_input=history_desired_input,\n",
        "    history_reference_model=history_reference_model,\n",
        "    history_controlled_object=history_controlled_object,\n",
        "    history_mean_error=history_mean_error,\n",
        "    history_reward=history_reward, \n",
        "    name='train'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count = np.count_nonzero(history_reward == -100)\n",
        "\n",
        "print(\"Number of occurrences of -100:\", count)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zSHiaIsp_hxu",
        "Fy_Ly0wC_Yiv",
        "n6oG4o52CyUV",
        "7u6gHkN0qTzc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
