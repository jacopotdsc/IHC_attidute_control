{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions import Normal\n",
        "from torchsummary import summary\n",
        "\n",
        "from kan import *\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import struct\n",
        "import socket\n",
        "\n",
        "import matlab.engine\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "global_var = {\n",
        "\n",
        "    # Connection\n",
        "    'ip_address': '127.0.0.1',                  # Ip to connect\n",
        "    'port_input': 8280,                         # Port where matlab send data\n",
        "    'port_output': 8281,                        # Port where matlab receive data\n",
        "    'port_f_input': 8282,                       # DEPRECATED: Same but for feedback branch\n",
        "    'port_f_output': 8283,                      # DEPRECATED: Same but for feedback branch\n",
        "    'buffer_size': 32,                          # Number of bytes to read \n",
        "    'connection_dim_input': 4,                  # Number of data to read -> used to unpack from byte, recevied data are byte\n",
        "    'connection_dim_output': 1,                 # Number of data in output -> used to pack to byte and sedn to matlab\n",
        "    'stop_flag': [-999, -999, -999, -999],      # DEPRECATED\n",
        "    'use_pid': True,                            # DEPRECATED\n",
        "        \n",
        "    # Matlab\n",
        "    'matlab_path': r'C:/Users/pc/Desktop/Artificial Intellingence & Robotics/1y-2s/Intelligent and hybrid control/IHC_attidute_control/control_schemas',\n",
        "    'simulation_name': 'pid_reinforcement.slx', \n",
        "    'simulation_time': 40.0,    # Time of the simulation\n",
        "    'open_GUI': False,          # If True, it open the simulink schema\n",
        "\n",
        "    # Dataset\n",
        "    'max_len_dataset': 10000,\n",
        "\n",
        "    # Feed Forward Network \n",
        "    'input_dim': 4,               # Dimension of input layer\n",
        "    'output_dim': 1,              # Dimension of output layer\n",
        "    'hidden_dim': 10,             # Dimension of hidden layer\n",
        "    'standard_deviation': 0.1,\n",
        "    'output_range': 5,           # Max control value\n",
        "    'bias': True,                \n",
        "    'learning_rate': 0.0001,       \n",
        "    'model_name': 'model.pt',     # Used to save weights\n",
        "    'model_path': 'weights/',     # Folder where save weights\n",
        "\n",
        "    # Train\n",
        "    'reward_control_factor': 0.02,  # facotr which damp the control. if 0 -> r=-e^2\n",
        "    'batch_size': 32,             # Batch size for training\n",
        "    'start_train_size': 1,        # DEPRECATED: Minium number of sample to traine\n",
        "    'n_episode': 500,                   \n",
        "    'frequency_update': 32,       # Frequency update of the network\n",
        "    'threshold_error': 2.5,    \n",
        "    'max_reward': -0.5, \n",
        "    'epsilon': 0.3,  \n",
        "    'max_abs_control': 5.0, \n",
        "\n",
        "    # Plots\n",
        "    'plots_path': 'plots/'        # Folder where save plots\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "class Color:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "    MAGENTA = '\\033[95m'\n",
        "    CYAN = '\\033[96m'\n",
        "    WHITE = '\\033[97m'\n",
        "    RESET = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actor-Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lr, bias):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        #self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.loss_fnc = nn.MSELoss()\n",
        "        \n",
        "        self.layer_1 = nn.Linear(\n",
        "            in_features=self.input_dim,\n",
        "            out_features=self.hidden_dim,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        '''\n",
        "        self.layer_2 = nn.Linear(\n",
        "            in_features=self.hidden_dim,\n",
        "            out_features=self.hidden_dim*2,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.layer_3 = nn.Linear(\n",
        "            in_features=self.hidden_dim*2,\n",
        "            out_features=self.hidden_dim*2,\n",
        "            bias=bias\n",
        "        )\n",
        "        '''\n",
        "        self.layer_out= nn.Linear(\n",
        "            in_features=self.hidden_dim,\n",
        "            out_features=1,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        state = self.layer_1(state)\n",
        "        state = self.tanh(state)\n",
        "        \n",
        "        '''\n",
        "        state = self.layer_2(state)\n",
        "        state = self.tanh(state)\n",
        "\n",
        "        state = self.layer_3(state)\n",
        "        state = self.tanh(state)\n",
        "        '''\n",
        "        state = self.layer_out(state)\n",
        "        \n",
        "        return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, lr, bias):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.loss_fnc = nn.MSELoss()\n",
        "        \n",
        "\n",
        "        self.layer_1 = KAN(width=[self.input_dim, self.hidden_dim])\n",
        "        \n",
        "        self.kp = KAN(width=[self.hidden_dim, self.hidden_dim])\n",
        "\n",
        "        self.ki = KAN(width=[self.hidden_dim, self.hidden_dim])\n",
        "\n",
        "        self.kd = KAN(width=[self.hidden_dim, self.hidden_dim])\n",
        "        \n",
        "        self.layer_out = KAN(width=[self.hidden_dim*3, self.output_dim])\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        state = self.layer_1(state)\n",
        "        state = self.tanh(state)\n",
        "        \n",
        "        state_kp = self.kp(state) + state\n",
        "        state_kp = self.relu(state_kp)\n",
        "\n",
        "        state_ki = self.ki(state) + state\n",
        "        state_ki = self.relu(state_ki)\n",
        "\n",
        "        state_kd = self.kd(state) + state\n",
        "        state_kd = self.relu(state_kd)\n",
        "        \n",
        "        state_conc = torch.cat([state_kp, state_ki, state_kd], 1 )\n",
        "\n",
        "        state = self.layer_out( state_conc )\n",
        "        mu = self.tanh(state)*global_var['max_abs_control']\n",
        "\n",
        "        return mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, bias, std, lr, model_name):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.critic = Critic(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            bias=bias,\n",
        "            lr=lr\n",
        "            )\n",
        "\n",
        "        self.actor = Actor(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            output_dim=output_dim,\n",
        "            bias=bias,\n",
        "            lr=lr\n",
        "            )\n",
        "\n",
        "        self.log_std = nn.Parameter(torch.ones(1, output_dim) * std)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.loss_fnc = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        value = self.critic(x)\n",
        "        mu    = self.actor(x)\n",
        "        std = self.log_std.exp().expand_as(mu)\n",
        "\n",
        "        action_distribution  = Normal(mu, std)\n",
        "        \n",
        "        action = action_distribution.sample()\n",
        "        raw_log_action = action_distribution.log_prob(action) # log probability of an action given the distribution\n",
        "\n",
        "        log_action = raw_log_action - torch.log( 1 - self.tanh(action)**2 + 1e-6  ) + torch.log( torch.tensor(global_var['max_abs_control']) )\n",
        "\n",
        "        # In statistic  given a random variable X, a transformation Y = f(X) is another random variable,\n",
        "        # the probability of Y is Py(Y) = Px( f(y)^-1 )*| D[f(y)^-1]_y |\n",
        "\n",
        "        # the transformation of log probability with tansh is the following:\n",
        "        # log( P ) = log( Pa ) - log( D[ tanh(a) ]_a ), where D[ tanh(a) ]_a = 1 - tanh(a)^2\n",
        "        # log( Pa ) is the probability density of a with the normal distribution\n",
        "\n",
        "        return action, value, action_distribution, log_action\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        name = self.model_name\n",
        "        torch.save(self.state_dict(), name )\n",
        "        #print(f\"{Color.MAGENTA}Saved: {name}{Color.RESET}\")\n",
        "\n",
        "    def load(self):\n",
        "        name = self.model_name\n",
        "        try:\n",
        "            self.load_state_dict(torch.load(name) )\n",
        "            print(f\"{Color.MAGENTA}loaded: {name}{Color.RESET}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{Color.RED}Model not loaded{Color.RESET}\")\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_socket(ip_address, port_input):\n",
        "\n",
        "    #print(f\"{Color.YELLOW}ip: {ip_address}, port_input: {port_input}{Color.RESET}\")\n",
        "\n",
        "    try:\n",
        "        socket_nn_input = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        socket_nn_input.bind( (ip_address, port_input) )\n",
        "        socket_nn_input.listen(1)\n",
        "\n",
        "        #print(f\"{Color.BOLD}{Color.GREEN}Sockets listener created{Color.RESET}\")\n",
        "\n",
        "        return socket_nn_input\n",
        "    except Exception as e:\n",
        "        print(f\"{Color.RED}An error occurred: {e}{Color.RESET}\")\n",
        "\n",
        "        if 'socket_nn_input' in locals():\n",
        "            socket_nn_input.close()\n",
        "            \n",
        "        return None, None\n",
        "    \n",
        "def accept_connection(socket_server):\n",
        "    connection, address = socket_server.accept()\n",
        "    #print(f\"{Color.GREEN}Connection accepted!{Color.RESET}\")\n",
        "    return connection, address\n",
        "\n",
        "def receive_data(connection, buffer_size, dim_input):\n",
        "    \n",
        "    expected_bytes = buffer_size  # Size for one double\n",
        "    no_data_flag = False\n",
        "    data = b''\n",
        "    while len(data) < expected_bytes:\n",
        "        more_data = connection.recv(expected_bytes - len(data))\n",
        "\n",
        "        if not more_data:\n",
        "            print(\"\\nNo data received. Ending connection.\")\n",
        "            no_data_flag = True\n",
        "            break\n",
        "\n",
        "        data += more_data\n",
        "    data = list(struct.unpack(f'!{str(dim_input)}d', data))  # Unpack one double\n",
        "    \n",
        "    return data, no_data_flag\n",
        "\n",
        "def receive_data_excpt(connection, buffer_size, dim_input, stop_flag):\n",
        "    \n",
        "    expected_bytes = 8# buffer_size  # Size for one double\n",
        "    data = b''\n",
        "    #connection.settimeout(5.0)  # Set timeout to 5 seconds\n",
        "\n",
        "    try:\n",
        "        while len(data) < expected_bytes:\n",
        "            #more_data = connection.recv(expected_bytes - len(data))\n",
        "            more_data = connection.recv(expected_bytes)\n",
        "            if not more_data:\n",
        "                # No more data is available, break the loop\n",
        "                break\n",
        "            data += more_data\n",
        "        try:\n",
        "            #data = list(struct.unpack(f'!{str(dim_input)}d', data)) \n",
        "            data = list(struct.unpack(f'!d', data)) \n",
        "        except Exception as e:\n",
        "            print(f\"\\n{e}\")\n",
        "            print(f\"{Color.RED}\\nProblem with unpacking, error: {e}{Color.RESET}\")\n",
        "            print(f\"{Color.RED}May be due because return empty string when nothing is receive for a certain time{Color.RESET}\")\n",
        "            return stop_flag\n",
        "        \n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"\\n{e}\")\n",
        "        if isinstance(e, socket.timeout):\n",
        "            print(f\"{Color.RED}\\nTimeout error: No data received within the timeout period{Color.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Color.RED}\\nOther exception occurred: {e}{Color.RESET}\")\n",
        "            print(f\"{Color.RED}Maybe due to some other error in the code{Color.RESET}\")\n",
        "            \n",
        "        print(f\"{Color.RED}May be due because return empty string when nothing is receive for a certain time{Color.RESET}\")\n",
        "        print(f\"{Color.BLUE}\\nNo data received within the timeout period, maybe some error of code{Color.RESET}\")\n",
        "       \n",
        "        return stop_flag\n",
        "    \n",
        "    connection.settimeout(None)\n",
        "    return data\n",
        "\n",
        "def send_data(connection, message, dim_output):\n",
        "    try:\n",
        "        #message_to_send = struct.pack(f'!{str(dim_output)}d', *message)  \n",
        "        message_to_send = struct.pack(f'!{str(dim_output)}d', message)\n",
        "        connection.sendall(message_to_send)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\rError sending float: {e}\", end='')\n",
        "\n",
        "def close_connections(*to_close):\n",
        "    try:\n",
        "        for c in to_close:\n",
        "            c.close()\n",
        "    except Exception as e:\n",
        "        print(f\"{Color.RED}\\nFailed to close connection: verified the follwing error:\\n{e}{Color.RESET}\")\n",
        "        return\n",
        "\n",
        "    #print(f\"{Color.GREEN}\\nSockets closed!{Color.RESET}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_simulink(model_name, matlab_path, simulation_time):\n",
        "\n",
        "    model_name = model_name.split('.')[0]\n",
        "\n",
        "    #print(f\"\\nStarting Matlab engine ... \", end='')\n",
        "    future = matlab.engine.start_matlab(background=True)\n",
        "    eng = future.result()  \n",
        "    #print(f\"Started!\")\n",
        "\n",
        "    eng.cd(matlab_path)\n",
        "    #print(\"MATLAB working directory:\", eng.pwd())\n",
        "\n",
        "    if global_var['open_GUI'] == False:\n",
        "        eng.load_system(model_name)\n",
        "    else:\n",
        "        eng.open_system(model_name, nargout=0)\n",
        "        \n",
        "    loaded_model = eng.bdroot()\n",
        "    #print(f\"The currently loaded model is: {loaded_model}\")\n",
        "\n",
        "    eng.set_param(model_name, 'StartTime', '0', nargout=0)\n",
        "    eng.set_param(model_name, 'StopTime', str(simulation_time), nargout=0)\n",
        "    eng.set_param(model_name, 'SimulationCommand', 'start', nargout=0)\n",
        "\n",
        "    return eng\n",
        "\n",
        "def run_simulink_with_gui(model_name, matlab_path, simulation_time):\n",
        "\n",
        "    model_name = model_name.split('.')[0]\n",
        "\n",
        "    #print(f\"\\nStarting Matlab engine ... \", end='')\n",
        "    future = matlab.engine.start_matlab(background=True)\n",
        "    eng = future.result()  \n",
        "    #print(f\"Started!\")\n",
        "\n",
        "    eng.cd(matlab_path)\n",
        "    #print(\"MATLAB working directory:\", eng.pwd())\n",
        "\n",
        "    eng.open_system(model_name, nargout=0)\n",
        "    loaded_model = eng.bdroot()\n",
        "\n",
        "    #print(f\"The currently loaded model is: {loaded_model}\")\n",
        "\n",
        "    eng.set_param(model_name, 'StartTime', '0', nargout=0)\n",
        "    eng.set_param(model_name, 'StopTime', str(simulation_time), nargout=0)\n",
        "    eng.set_param(model_name, 'SimulationCommand', 'start', nargout=0)\n",
        "\n",
        "    return eng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, states, rewards, actions, log_action, next_states):\n",
        "\n",
        "    # converting array of tensor inot a tensor of tensor to create the stack for the batch\n",
        "    \n",
        "    states = torch.stack( states, dim=0 )\n",
        "    rewards = torch.stack( rewards, dim=0)\n",
        "    actions = torch.stack( actions, dim=0)\n",
        "    log_action = torch.stack( log_action, dim=0)\n",
        "    next_states = torch.stack( next_states, dim=0)\n",
        "\n",
        "    #states = states.view( states.shape[0], -1)\n",
        "    #rewards = rewards.view( rewards.shape[0], -1)\n",
        "    #actions = actions.view( actions.shape[0] , -1)\n",
        "    #log_action = log_action.view( log_action.shape[0], -1)\n",
        "    #next_states = next_states.view( next_states.shape[0], -1)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss = 0\n",
        "    gamma = 0.99\n",
        "\n",
        "    model.critic.optimizer.zero_grad()\n",
        "    model.actor.optimizer.zero_grad()\n",
        "\n",
        "    ##### start calculating loss #####\n",
        "\n",
        "    #dist, values = model(states)\n",
        "    _, values, _, _ = model(states)\n",
        "\n",
        "    value_next_state = model.critic(next_states).detach()\n",
        "    label = rewards + gamma * value_next_state\n",
        "    delta =  label - values \n",
        "\n",
        "    critic_loss = model.critic.loss_fnc(delta, torch.zeros(delta.shape) ) \n",
        "    actor_loss = torch.mean(delta * log_action )\n",
        "    #actor_loss = actor_loss.mean()\n",
        "\n",
        "    #print(rewards.shape)\n",
        "    #print(values.shape)\n",
        "    #print(value_next_state.shape)\n",
        "    #print(delta)\n",
        "    #print(f\"Delta: {delta.mean():.3f}, actions: {actions.mean():.3f}, log_act: {log_action.mean():.3f}, Critic: {critic_loss:.3f}, Actor: {actor_loss:.3f}\")\n",
        "\n",
        "    ##### End calculating loss #####\n",
        "\n",
        "    actor_loss.backward(retain_graph=True)\n",
        "    critic_loss.backward()\n",
        "\n",
        "    model.critic.optimizer.step()\n",
        "    model.actor.optimizer.step()\n",
        "\n",
        "    model.save()\n",
        "    model.eval()\n",
        "\n",
        "    print()\n",
        "    print(\"*****\")\n",
        "    print(f\"val_state: {values.shape}, {values.mean()}\")\n",
        "    print(f\"val_next_state: {value_next_state.shape}, {value_next_state.mean()}\")\n",
        "    print(f\"label: {label.shape}, {label.mean()}\")\n",
        "    print(f\"delta: {delta.shape}, {delta.mean()} -> delta^2: {delta.mean()**2}\")\n",
        "    print(f\"log_action: {log_action.shape}, {log_action.mean()}\")\n",
        "    print(f\"Actor_loss: {actor_loss}\")\n",
        "    print(f\"Critic_loss: {critic_loss}\")\n",
        "\n",
        "    return actor_loss, critic_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_statistics(history_error, history_desired_input, history_reference_model, history_controlled_object, history_mean_error, history_reward, name):\n",
        "    \n",
        "\n",
        "    time = np.linspace(0, int(global_var['simulation_time']), len(history_error))\n",
        "    time_reward = np.linspace(0, int(global_var['simulation_time']), len(history_reward))\n",
        "\n",
        "    fig, axs = plt.subplots(2,3,figsize=(18, 10))\n",
        "\n",
        "    axs[0, 0].plot(time, history_desired_input)\n",
        "    axs[0, 0].set_title('Desired input')\n",
        "    axs[0, 0].grid(True)\n",
        "\n",
        "    axs[0, 1].plot(time, history_reference_model)\n",
        "    axs[0, 1].set_title('Reference model')\n",
        "    axs[0, 1].grid(True)\n",
        "\n",
        "    axs[1, 0].plot(time, history_controlled_object)\n",
        "    axs[1, 0].set_title('Controlled object')\n",
        "    axs[1, 0].grid(True)\n",
        "\n",
        "    axs[1, 1].plot(time_reward, history_reward)\n",
        "    axs[1, 1].set_title('Reward')\n",
        "    axs[1, 1].grid(True)\n",
        "\n",
        "    axs[0, 2].plot(time, history_error)\n",
        "    axs[0, 2].set_title('Error')\n",
        "    axs[0, 2].grid(True)\n",
        "\n",
        "\n",
        "    axs[1, 2].plot(time, history_mean_error)\n",
        "    axs[1, 2].set_title('Abs mean error - last 50')\n",
        "    axs[1, 2].grid(True)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_name = name + '_' + 'range_' + str(global_var['max_abs_control']) + '_sim_' + global_var['simulation_name'].split('.')[0] + '_' + str(int(global_var['simulation_time']))\n",
        "    print(fig_name)\n",
        "\n",
        "    plt.grid(True)\n",
        "    #plt.savefig(global_var['plots_path'] + fig_name)\n",
        "    plt.show(block=False)\n",
        "\n",
        "    \n",
        "    #plt.pause(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss(history_actor_loss, history_critic_loss, history_reward, history_error, name):\n",
        "\n",
        "\n",
        "    clear_output(True)\n",
        "   \n",
        "    time = np.linspace(0, len(history_actor_loss) - 1, len(history_actor_loss))\n",
        "    time_reward = np.linspace(0, len(history_reward) - 1, len(history_reward))\n",
        "    time_error = np.linspace(0, len(history_error) - 1, len(history_error))\n",
        "\n",
        "    fig_1, axs_1 = plt.subplots(1,2,figsize=(12, 5))\n",
        "    fig_2, axs_2 = plt.subplots(1,2,figsize=(12, 5))\n",
        "\n",
        "\n",
        "    #history_reward = torch.stack(history_reward).detach().cpu().numpy().flatten()\n",
        "    #history_error = torch.stack(history_error).detach().cpu().numpy().flatten()\n",
        "\n",
        "    axs_1[0].plot(time, history_actor_loss)\n",
        "    axs_1[0].set_title('Actor loss')\n",
        "    axs_1[0].grid(True)\n",
        "\n",
        "    axs_1[1].plot(time, history_critic_loss)\n",
        "    axs_1[1].set_title('Critic loss')\n",
        "    axs_1[1].grid(True)\n",
        "\n",
        "    axs_2[0].plot(time_reward, history_reward)\n",
        "    axs_2[0].set_title('Reward')\n",
        "    axs_2[0].grid(True)\n",
        "\n",
        "    axs_2[1].plot(time_error, history_error)\n",
        "    axs_2[1].set_title('Error')\n",
        "    axs_2[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # fig_name = name + '_' + 'range_' + str(global_var['max_abs_control']) + '_sim_' + global_var['simulation_name'].split('.')[0] + '_' + str(int(global_var['simulation_time']))\n",
        "    # print(fig_name)\n",
        "\n",
        "    plt.grid(True)\n",
        "    #plt.savefig(global_var['plots_path'] + fig_name)\n",
        "    plt.show(block=False)\n",
        "\n",
        "    #display(plt.gcf())  # Display the current figure\n",
        "    #clear_output(wait=True)  # Clear the output before the next plot\n",
        "    #plt.close(fig)\n",
        "\n",
        "    \n",
        "    \n",
        "    #plt.pause(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_reward(error, control):\n",
        "\n",
        "    error = error #/ global_var['threshold_error']\n",
        "    control = control#/ global_var['max_abs_control']\n",
        "\n",
        "    reward = abs(error) + abs(global_var['reward_control_factor']*(control.item()) )\n",
        "    reward = -reward\n",
        "\n",
        "    reward = max(reward, -abs(global_var['max_reward']) )\n",
        "    \n",
        "    reward = torch.tensor([reward], dtype=torch.float, requires_grad=True)\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_episode( connection_receiver, \n",
        "                  connection_sender, \n",
        "                  network, \n",
        "                  history_desired_input, \n",
        "                  history_reference_model,\n",
        "                  history_controlled_object,\n",
        "                  history_error,\n",
        "                  history_mean_error,\n",
        "                  history_reward,\n",
        "                  mean_error_window,\n",
        "                  train_flag\n",
        "                  ):\n",
        "    \n",
        "    n_received = 0\n",
        "    n_sent = 0\n",
        "    n_train = 0\n",
        "    mean_loss = 0\n",
        "    n_random = 0\n",
        "    actor_loss = 0.0\n",
        "    critic_loss = 0.0\n",
        "    penalize_flag = False\n",
        "    \n",
        "    states_array = []\n",
        "    actions_array = []\n",
        "    log_prob_array = []\n",
        "    rewards_array = []\n",
        "    error_array = []\n",
        "    next_states_array = []\n",
        "\n",
        "    loss_array = []\n",
        "    actor_loss_array = []\n",
        "    critic_loss_array = []\n",
        "\n",
        "    while True:\n",
        "\n",
        "        raw_data, _ = receive_data(\n",
        "            connection=connection_receiver, \n",
        "            buffer_size=global_var['buffer_size'],\n",
        "            dim_input=global_var['connection_dim_input']\n",
        "            )\n",
        "        n_received += 1\n",
        "        \n",
        "       \n",
        "        history_controlled_object.append(raw_data[0])\n",
        "        history_desired_input.append(raw_data[1])\n",
        "        history_reference_model.append(raw_data[2])\n",
        "        history_error.append(raw_data[-1])\n",
        "        history_mean_error.append(abs(np.mean(history_error[-mean_error_window:])))\n",
        "        \n",
        "    \n",
        "        if abs(raw_data[-1]) > global_var['threshold_error'] and n_received > 1: # end episode if diverging\n",
        "            penalize_flag = True\n",
        "        \n",
        "        raw_data = torch.tensor(raw_data, requires_grad=False)\n",
        "\n",
        "        if len(next_states_array) > 1 and ( len(next_states_array) % global_var['frequency_update'] ) == 0 and train_flag == True:\n",
        "            \n",
        "            current_loss = 0\n",
        "            mean_loss = 0\n",
        "            \n",
        "            actor_loss, critic_loss = train(\n",
        "                model=network,\n",
        "                states=states_array[-global_var['frequency_update']:],\n",
        "                actions=actions_array[-global_var['frequency_update']:],\n",
        "                log_action=log_prob_array[-global_var['frequency_update']:],\n",
        "                rewards=rewards_array[-global_var['frequency_update']:],\n",
        "                next_states=next_states_array[-global_var['frequency_update']:]\n",
        "            )  \n",
        "            n_train += 1\n",
        "            \n",
        "            #mean_loss = torch.mean( torch.stack(actor_loss + critic_loss, dim=0))\n",
        "\n",
        "            actor_loss = actor_loss.item()\n",
        "            critic_loss = critic_loss.item()\n",
        "            \n",
        "            actor_loss_array.append(actor_loss)\n",
        "            critic_loss_array.append(critic_loss)\n",
        "            loss_array.append(actor_loss + critic_loss)\n",
        "            \n",
        "            '''\n",
        "            if ( n_train % 10 ) == 0 or penalize_flag == True:\n",
        "                plot_loss(\n",
        "                    history_actor_loss=actor_loss_array,\n",
        "                    history_critic_loss=critic_loss_array,\n",
        "                    history_reward=rewards_array,\n",
        "                    history_error=error_array,\n",
        "                    name='Loss'\n",
        "                )\n",
        "            '''\n",
        "\n",
        "            if penalize_flag == True : # end episode if diverging\n",
        "                penalize_flag = False\n",
        "                print(f\"{Color.RED}\\n---- Error too high, Restarting episode{Color.RESET}\")\n",
        "                raise Exception(\"Error too high, Restarting episode\")\n",
        "            \n",
        "            \n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            network_input = raw_data.unsqueeze(0)\n",
        "            action, value, action_distribution, log_action = network(network_input)\n",
        "\n",
        "            action = action.squeeze(0)\n",
        "            value = value.squeeze(0)\n",
        "            log_action = log_action.squeeze(0)\n",
        "\n",
        "        \n",
        "        if random.random() < global_var['epsilon'] and train_flag == True:\n",
        "            action = torch.rand( action.shape )\n",
        "            action = torch.clip(action, min=-global_var['max_abs_control'], max=global_var['max_abs_control'])\n",
        "            action = action\n",
        "            n_random += 1\n",
        "        else:\n",
        "            action = action\n",
        "            \n",
        "        current_reward = compute_reward( raw_data[-1], action )\n",
        "\n",
        "\n",
        "        raw_data.requires_grad_(True)\n",
        "        action.requires_grad_(True)\n",
        "        current_reward.requires_grad_(True)\n",
        "\n",
        "        if penalize_flag == True:\n",
        "            #current_reward = current_reward\n",
        "            current_reward = torch.tensor([ -abs(global_var['max_reward'])*2.0], dtype=torch.float, requires_grad=True)\n",
        "\n",
        "        history_reward.append(current_reward.item())\n",
        "            \n",
        "        states_array.append( raw_data )\n",
        "        log_prob_array.append(log_action)\n",
        "        rewards_array.append( current_reward )\n",
        "        error_array.append( raw_data[-1] )\n",
        "        \n",
        "        \n",
        "\n",
        "        if not ( states_array == []):\n",
        "            current_next_state = states_array[-1].clone()\n",
        "            current_next_state.requires_grad_(True)\n",
        "            next_states_array.append( current_next_state )\n",
        "        \n",
        "        \n",
        "        \n",
        "        actions_array.append( action )\n",
        "\n",
        "        \n",
        "        #print(f\"raw_data: {raw_data}, shape: {raw_data.shape}\")\n",
        "        #print(f\"log_action: {log_action}, shape: {log_action.shape}\")\n",
        "        #print(f\"current_reward: {current_reward}, shape: {current_reward.shape}\")\n",
        "        #print(f\"action: {action}, shape: {action.shape}\")\n",
        " \n",
        "    \n",
        "        send_data(\n",
        "            connection=connection_sender, \n",
        "            message=action.item(), \n",
        "            dim_output=global_var['connection_dim_output']\n",
        "            )\n",
        "        n_sent += 1\n",
        "\n",
        "        output_line = (\n",
        "            f\"\\rreceived: {n_received}, sent: {n_sent}, n_train: {n_train}, \"\n",
        "            f\"error: {raw_data[-1]:.4f}, reward : {current_reward.item():.4f}\" #, actor_loss: {np.mean(actor_loss_array[-mean_error_window:]) }, critic_loss: {np.mean(critic_loss_array[-mean_error_window:])}\"\n",
        "            f\" mean_error_{str(mean_error_window)}: {np.mean(history_error[-mean_error_window:]):.4f} ,mean loss: {mean_loss:.4f}{' ' * 50}\"\n",
        "        )\n",
        "        \n",
        "        #print(output_line, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialization_connection_and_simulink( ip_address, \n",
        "                                            port_input, \n",
        "                                            port_output,\n",
        "                                            simulation_name,\n",
        "                                            simulation_time,\n",
        "                                            matlab_path\n",
        "                                            ):\n",
        "    receiver_socket = setup_socket(\n",
        "        ip_address=ip_address, \n",
        "        port_input=port_input\n",
        "        )\n",
        "\n",
        "    send_socket = setup_socket(\n",
        "        ip_address=ip_address, \n",
        "        port_input=port_output\n",
        "        )\n",
        "        \n",
        "\n",
        "    #eng = run_simulink_with_gui(model_name, eng_path, simulation_time)\n",
        "    \n",
        "    eng = run_simulink(\n",
        "        model_name=simulation_name, \n",
        "        matlab_path=matlab_path, \n",
        "        simulation_time=simulation_time\n",
        "        )\n",
        "\n",
        "\n",
        "    #print(f\"{Color.CYAN}\\nWaiting someone to connect ...{Color.RESET}\")\n",
        "    connection_receiver, addr = accept_connection(receiver_socket)\n",
        "    connection_sender, addr = accept_connection(send_socket)\n",
        "\n",
        "    return receiver_socket, send_socket, connection_receiver, connection_sender, eng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reinforcement_training_loop(\n",
        "        network,\n",
        "        history_desired_input,\n",
        "        history_reference_model,\n",
        "        history_controlled_object,\n",
        "        history_error,\n",
        "        history_mean_error,\n",
        "        mean_error_window,\n",
        "        history_reward, \n",
        "):    \n",
        "    try:\n",
        "\n",
        "        receiver_socket, send_socket, connection_receiver, connection_sender, eng = initialization_connection_and_simulink(\n",
        "                                                                                                                        ip_address=global_var['ip_address'],\n",
        "                                                                                                                        port_input=global_var['port_input'],\n",
        "                                                                                                                        port_output=global_var['port_output'],\n",
        "                                                                                                                        simulation_name=global_var['simulation_name'],\n",
        "                                                                                                                        simulation_time=global_var['simulation_time'],\n",
        "                                                                                                                        matlab_path=global_var['matlab_path']\n",
        "                                                                                                                    )\n",
        "        play_episode_v2(\n",
        "            connection_receiver=connection_receiver, \n",
        "            connection_sender=connection_sender, \n",
        "            network=network, \n",
        "            history_desired_input=history_desired_input, \n",
        "            history_reference_model=history_reference_model,\n",
        "            history_controlled_object=history_controlled_object,\n",
        "            history_error=history_error,\n",
        "            history_mean_error=history_mean_error,\n",
        "            history_reward=history_reward,\n",
        "            mean_error_window=mean_error_window,\n",
        "            train_flag = True\n",
        "            )\n",
        "       \n",
        "        \n",
        "    except Exception as e:\n",
        "        print()\n",
        "        print(e)\n",
        "        pass\n",
        "\n",
        "    finally:\n",
        "\n",
        "        close_connections(\n",
        "            connection_receiver,\n",
        "            connection_sender,\n",
        "            send_socket,\n",
        "            receiver_socket,\n",
        "        )\n",
        "\n",
        "        eng.quit()\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_reinforcement_loop(\n",
        "        network,\n",
        "        history_desired_input,\n",
        "        history_reference_model,\n",
        "        history_controlled_object,\n",
        "        history_error,\n",
        "        history_mean_error,\n",
        "        history_reward,\n",
        "        mean_error_window,\n",
        "): \n",
        "    for e in range(global_var['n_episode']):\n",
        "        print(f\"\\rn_episode: {e}\\n\", end=\"\")\n",
        "\n",
        "        history_desired_input = []\n",
        "        history_reference_model = []\n",
        "        history_controlled_object = []\n",
        "        history_error = []\n",
        "        history_mean_error = []\n",
        "        history_reward = []\n",
        "\n",
        "        reinforcement_training_loop(\n",
        "                network=network,\n",
        "                history_desired_input=history_desired_input,\n",
        "                history_reference_model=history_reference_model,\n",
        "                history_controlled_object=history_controlled_object,\n",
        "                history_error=history_error,\n",
        "                history_mean_error=history_mean_error,\n",
        "                history_reward=history_reward,\n",
        "                mean_error_window=mean_error_window,\n",
        "        )\n",
        "\n",
        "        if (e % 50) == 0:\n",
        "            network.save()     \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO Algorithm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalization function for range [0, 1]\n",
        "def normalize_01(x, min_val, max_val):\n",
        "    return (x - min_val) / (max_val - min_val)\n",
        "\n",
        "# Normalization function for range [-1, 1]\n",
        "def normalize_m11(x, min_val, max_val):\n",
        "    return 2 * (x - min_val) / (max_val - min_val) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ppo_train(model, state_memory, action_memory, reward_memory, log_prog_memory, state_value_memory, next_state_memory):\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    state_memory = torch.stack(state_memory, dim=0)\n",
        "    action_memory = torch.stack(action_memory, dim=0)\n",
        "    reward_memory = torch.stack(reward_memory, dim=0)\n",
        "    log_prog_memory = torch.stack(log_prog_memory, dim=0)\n",
        "    state_value_memory = torch.stack(state_value_memory, dim=0)\n",
        "    next_state_memory = torch.stack(next_state_memory, dim=0)\n",
        "\n",
        "    model.critic.optimizer.zero_grad()\n",
        "    model.actor.optimizer.zero_grad()\n",
        "\n",
        "    ### calculating loss\n",
        "    delta = reward_memory + 0.99*next_state_memory - state_memory\n",
        "    \n",
        "    critic_loss = model.critic.loss_fnc(delta, torch.zeros(delta.shape) )\n",
        "    actor_loss = -(log_prog_memory * delta.detach()).mean()\n",
        "\n",
        "    ### optimizing\n",
        "    actor_loss.backward(retain_graph=True)\n",
        "    critic_loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    model.critic.optimizer.step()\n",
        "    model.actor.optimizer.step()\n",
        "\n",
        "    model.save()\n",
        "    model.eval()\n",
        "\n",
        "    return actor_loss, critic_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_reward_v2(error, delta_error, control):\n",
        "\n",
        "    sign = 1\n",
        "    if delta_error > 0:\n",
        "        sign = 1\n",
        "    elif delta_error <= 0:\n",
        "        sign = -1\n",
        "\n",
        "    error_norm = error / abs(global_var['threshold_error'])\n",
        "    control_norm = control / abs(global_var['max_abs_control'])\n",
        "\n",
        "    max_reward = abs(global_var['max_reward'])\n",
        "    current_reward = abs(error_norm + 0.02 * control_norm)\n",
        "    #current_reward = np.clip(current_reward, -max_reward, max_reward )\n",
        "\n",
        "\n",
        "    if sign == 1:\n",
        "        if abs(error) <= 0.25:\n",
        "            reward = max_reward \n",
        "        elif abs(error) <= 0.5:\n",
        "            reward =  max_reward - current_reward\n",
        "        elif abs(error) <= 1:\n",
        "            reward = max_reward - current_reward * 1.\n",
        "        else:\n",
        "            reward = max_reward - current_reward * 1.5\n",
        "\n",
        "    elif sign == -1:\n",
        "        if abs(error) <= 0.25:\n",
        "            reward = sign * max_reward \n",
        "        elif abs(error) <= 0.5:\n",
        "            reward =  sign*(max_reward - current_reward)\n",
        "        elif abs(error) <= 1:\n",
        "            reward = sign*(max_reward - current_reward * 2)\n",
        "        elif abs(error) >= 1.5 :\n",
        "            reward = current_reward * 1.5\n",
        "        elif abs(error) >= 2 :\n",
        "            reward = current_reward * 2\n",
        "        else:\n",
        "            reward = current_reward\n",
        "\n",
        "\n",
        "    reward = np.clip(sign*reward, -max_reward, max_reward )\n",
        "\n",
        "    reward = torch.tensor([[reward]], dtype=torch.float, requires_grad=True)\n",
        "\n",
        "    #print(f\"error: {error} \")\n",
        "    #print(f\"reward: {reward}\")\n",
        "    #print(f\"delta_error: {delta_error}, sign: {sign}\\n\")\n",
        "    \n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_episode_v2( \n",
        "                    connection_receiver, \n",
        "                    connection_sender, \n",
        "                    network,\n",
        "                    history_desired_input, \n",
        "                    history_reference_model,\n",
        "                    history_controlled_object,\n",
        "                    history_error,\n",
        "                    history_mean_error,\n",
        "                    history_reward,\n",
        "                    mean_error_window,\n",
        "                    train_flag = True\n",
        "                  ):\n",
        "    \n",
        "    done = False\n",
        "    state_memory = []\n",
        "    action_memory = []\n",
        "    reward_memory = []\n",
        "    log_prob_memory = []\n",
        "    state_value_memory = []\n",
        "    next_state_memory = []\n",
        "\n",
        "    history_actor_loss = []\n",
        "    history_critic_loss = []\n",
        "    history_error = []\n",
        "    history_reward = []\n",
        "\n",
        "    n_update = 0\n",
        "\n",
        "    ### Receiving first state\n",
        "    raw_state, _ = receive_data(\n",
        "        connection=connection_receiver, \n",
        "        buffer_size=global_var['buffer_size'],\n",
        "        dim_input=global_var['connection_dim_input']\n",
        "        )\n",
        "        \n",
        "    \n",
        "    history_controlled_object.append(raw_state[0])\n",
        "    history_desired_input.append(raw_state[1])\n",
        "    history_reference_model.append(raw_state[2])\n",
        "    history_error.append(raw_state[-1])\n",
        "    history_mean_error.append(abs(np.mean(history_error[-mean_error_window:])))\n",
        "\n",
        "    state = torch.tensor( raw_state, dtype=torch.float32)\n",
        "    old_error = state[-1]\n",
        "\n",
        "    while done == False:\n",
        "\n",
        "        for _ in range( global_var['batch_size'] ):\n",
        "\n",
        "            val_abs_state = 30\n",
        "            \n",
        "            #print(f\"old_error: {old_error}\")\n",
        "            state = state.unsqueeze(0)\n",
        "            #state = normalize_m11(state, -val_abs_state, val_abs_state )\n",
        "    \n",
        "\n",
        "            ### Calculating value for this state\n",
        "            action, state_value, action_distribution, log_prob = network(state)\n",
        "            \n",
        "\n",
        "            ### Going to the next state\n",
        "            send_data(\n",
        "                connection=connection_sender, \n",
        "                message=action.item(), \n",
        "                dim_output=global_var['connection_dim_output']\n",
        "            )\n",
        "\n",
        "            next_raw_data, _ = receive_data(\n",
        "                connection=connection_receiver, \n",
        "                buffer_size=global_var['buffer_size'],\n",
        "                dim_input=global_var['connection_dim_input']\n",
        "                )\n",
        "\n",
        "            history_controlled_object.append(next_raw_data[0])\n",
        "            history_desired_input.append(next_raw_data[1])\n",
        "            history_reference_model.append(next_raw_data[2])\n",
        "            #history_error.append(next_raw_data[-1])\n",
        "            history_mean_error.append(abs(np.mean(history_error[-mean_error_window:])))\n",
        "\n",
        "            \n",
        "            next_state = torch.tensor(next_raw_data, dtype=torch.float32)\n",
        "            error = next_state[-1].item()\n",
        "            \n",
        "           # next_state = normalize_m11(next_state, -val_abs_state, val_abs_state)\n",
        "\n",
        "            ### check condition of termination\n",
        "            if abs(error) > global_var['threshold_error']:\n",
        "                done = True\n",
        "\n",
        "            ### Computing rewward\n",
        "            delta_error = error - old_error\n",
        "            reward = compute_reward_v2( error, delta_error, action)\n",
        "            #print(f\"return reward: {reward}\\n\")\n",
        "            \n",
        "            \n",
        "            history_error.append( error )\n",
        "            history_reward.append(reward.item())\n",
        "\n",
        "\n",
        "            ### adjusting dimension and appending\n",
        "            state = state.squeeze(0)\n",
        "            action = action.squeeze(0)\n",
        "            log_prob = log_prob.squeeze(0)\n",
        "            state_value = state_value.squeeze(0)\n",
        "\n",
        "            state_memory.append(state)\n",
        "            action_memory.append(action)\n",
        "            reward_memory.append(reward)\n",
        "            log_prob_memory.append(log_prob)\n",
        "            state_value_memory.append(state_value)\n",
        "            next_state_memory.append(next_state)\n",
        "\n",
        "            ### Iterating\n",
        "            state = next_state.clone().detach() \n",
        "\n",
        "            old_error = error\n",
        "\n",
        "        ### Update of network\n",
        "        actor_loss, critic_loss = ppo_train(\n",
        "                                            model=network,\n",
        "                                            state_memory=state_memory,\n",
        "                                            action_memory=action_memory,\n",
        "                                            reward_memory=reward_memory,\n",
        "                                            log_prog_memory=log_prob_memory,\n",
        "                                            state_value_memory=state_value_memory,\n",
        "                                            next_state_memory=next_state_memory\n",
        "                                        )\n",
        "        n_update += 1\n",
        "\n",
        "        history_actor_loss.append( actor_loss.item() )\n",
        "        history_critic_loss.append( critic_loss.item() ) \n",
        "\n",
        "        ### Clear memory\n",
        "        state_memory = []\n",
        "        action_memory = []\n",
        "        reward_memory = []\n",
        "        log_prob_memory = []\n",
        "        state_value_memory = []\n",
        "        next_state_memory = []\n",
        "\n",
        "\n",
        "        \n",
        "        if ( n_update % 10 ) == 0 or done == True :\n",
        "\n",
        "            '''\n",
        "            plot_loss(\n",
        "                history_actor_loss=history_actor_loss,\n",
        "                history_critic_loss=history_critic_loss,\n",
        "                history_reward=history_reward,\n",
        "                history_error=history_error,\n",
        "                name='Loss'\n",
        "            )\n",
        "            '''\n",
        "            plot_statistics(\n",
        "                history_error=history_error,\n",
        "                history_desired_input=history_desired_input,\n",
        "                history_reference_model=history_reference_model,\n",
        "                history_controlled_object=history_controlled_object,\n",
        "                history_mean_error=history_mean_error,\n",
        "                history_reward=history_reward,\n",
        "                name='train'\n",
        "            )\n",
        "        \n",
        "        \n",
        "        #(history_error,\n",
        "        #  history_desired_input, \n",
        "        # history_reference_model, \n",
        "        # history_controlled_object, \n",
        "        # history_mean_error, \n",
        "        # history_reward,\n",
        "        #  name)        \n",
        "\n",
        "    if done == True : \n",
        "        print(f\"{Color.RED}\\n---- Error too high, Restarting episode{Color.RESET}\")\n",
        "        raise Exception(\"Error too high, Restarting episode\")\n",
        "    \n",
        "    network.save()\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "network = ActorCritic(  \n",
        "                        input_dim=global_var['input_dim'],\n",
        "                        output_dim=global_var['output_dim'],\n",
        "                        hidden_dim=global_var['hidden_dim'],\n",
        "                        bias=global_var['bias'],\n",
        "                        #output_range=global_var['output_range'],\n",
        "                        std=global_var['standard_deviation'],\n",
        "                        lr=global_var['learning_rate'],\n",
        "                        model_name=global_var['model_path']+global_var['model_name']\n",
        "                    )\n",
        "#network.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://simple-pid.readthedocs.io/en/latest/reference.html\n",
        "\n",
        "n_received = 0\n",
        "n_sent = 0\n",
        "previous_error = 0\n",
        "\n",
        "history_desired_input = []\n",
        "history_reference_model = []\n",
        "history_controlled_object = []\n",
        "history_error = []\n",
        "history_mean_error = []\n",
        "history_reward = []\n",
        "\n",
        "mean_error_window = 50\n",
        "mean_loss = 0\n",
        "n_train = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[91mModel not loaded\u001b[0m\n",
            "Error(s) in loading state_dict for ActorCritic:\n",
            "\tMissing key(s) in state_dict: \"actor.layer_1.biases.0.weight\", \"actor.layer_1.act_fun.0.grid\", \"actor.layer_1.act_fun.0.coef\", \"actor.layer_1.act_fun.0.scale_base\", \"actor.layer_1.act_fun.0.scale_sp\", \"actor.layer_1.act_fun.0.mask\", \"actor.layer_1.symbolic_fun.0.mask\", \"actor.layer_1.symbolic_fun.0.affine\", \"actor.kp.biases.0.weight\", \"actor.kp.act_fun.0.grid\", \"actor.kp.act_fun.0.coef\", \"actor.kp.act_fun.0.scale_base\", \"actor.kp.act_fun.0.scale_sp\", \"actor.kp.act_fun.0.mask\", \"actor.kp.symbolic_fun.0.mask\", \"actor.kp.symbolic_fun.0.affine\", \"actor.ki.biases.0.weight\", \"actor.ki.act_fun.0.grid\", \"actor.ki.act_fun.0.coef\", \"actor.ki.act_fun.0.scale_base\", \"actor.ki.act_fun.0.scale_sp\", \"actor.ki.act_fun.0.mask\", \"actor.ki.symbolic_fun.0.mask\", \"actor.ki.symbolic_fun.0.affine\", \"actor.kd.biases.0.weight\", \"actor.kd.act_fun.0.grid\", \"actor.kd.act_fun.0.coef\", \"actor.kd.act_fun.0.scale_base\", \"actor.kd.act_fun.0.scale_sp\", \"actor.kd.act_fun.0.mask\", \"actor.kd.symbolic_fun.0.mask\", \"actor.kd.symbolic_fun.0.affine\", \"actor.layer_out.biases.0.weight\", \"actor.layer_out.act_fun.0.grid\", \"actor.layer_out.act_fun.0.coef\", \"actor.layer_out.act_fun.0.scale_base\", \"actor.layer_out.act_fun.0.scale_sp\", \"actor.layer_out.act_fun.0.mask\", \"actor.layer_out.symbolic_fun.0.mask\", \"actor.layer_out.symbolic_fun.0.affine\". \n",
            "\tUnexpected key(s) in state_dict: \"actor.layer_1.weight\", \"actor.layer_1.bias\", \"actor.kp.weight\", \"actor.kp.bias\", \"actor.ki.weight\", \"actor.ki.bias\", \"actor.kd.weight\", \"actor.kd.bias\", \"actor.layer_out.weight\", \"actor.layer_out.bias\". \n"
          ]
        }
      ],
      "source": [
        "network.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_episode: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "description:   0%|                                                          | 0/100 [01:30<?, ?it/s]\n",
            "description:   0%|                                                          | 0/100 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "'bool' object is not subscriptable\n",
            "n_episode: 1\n",
            "\n",
            "Invalid Simulink object name: 'pid_reinforcement'.\n",
            "\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "local variable 'connection_receiver' referenced before assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplay_reinforcement_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_desired_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_desired_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_reference_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_reference_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_controlled_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_controlled_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_mean_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_mean_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_reward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmean_error_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_error_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[35], line 21\u001b[0m, in \u001b[0;36mplay_reinforcement_loop\u001b[1;34m(network, history_desired_input, history_reference_model, history_controlled_object, history_error, history_mean_error, history_reward, mean_error_window)\u001b[0m\n\u001b[0;32m     18\u001b[0m history_mean_error \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m history_reward \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 21\u001b[0m \u001b[43mreinforcement_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_desired_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_desired_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_reference_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_reference_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_controlled_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_controlled_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_mean_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_mean_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_reward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmean_error_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_error_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     33\u001b[0m     network\u001b[38;5;241m.\u001b[39msave()\n",
            "Cell \u001b[1;32mIn[34], line 44\u001b[0m, in \u001b[0;36mreinforcement_training_loop\u001b[1;34m(network, history_desired_input, history_reference_model, history_controlled_object, history_error, history_mean_error, mean_error_window, history_reward)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     close_connections(\n\u001b[1;32m---> 44\u001b[0m         \u001b[43mconnection_receiver\u001b[49m,\n\u001b[0;32m     45\u001b[0m         connection_sender,\n\u001b[0;32m     46\u001b[0m         send_socket,\n\u001b[0;32m     47\u001b[0m         receiver_socket,\n\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     50\u001b[0m     eng\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'connection_receiver' referenced before assignment"
          ]
        }
      ],
      "source": [
        "play_reinforcement_loop(\n",
        "        network=network,\n",
        "        history_desired_input=history_desired_input,\n",
        "        history_reference_model=history_reference_model,\n",
        "        history_controlled_object=history_controlled_object,\n",
        "        history_error=history_error,\n",
        "        history_mean_error=history_mean_error,\n",
        "        history_reward=history_reward,\n",
        "        mean_error_window=mean_error_window,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "network.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_statistics(\n",
        "    history_error=history_error,\n",
        "    history_desired_input=history_desired_input,\n",
        "    history_reference_model=history_reference_model,\n",
        "    history_controlled_object=history_controlled_object,\n",
        "    history_mean_error=history_mean_error,\n",
        "    history_reward=history_reward, \n",
        "    name='train'\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zSHiaIsp_hxu",
        "Fy_Ly0wC_Yiv",
        "n6oG4o52CyUV",
        "7u6gHkN0qTzc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
